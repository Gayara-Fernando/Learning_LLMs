{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92fc7113-b0b9-4ad4-9c44-e8bbc2353272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbaa78f3-6f51-451b-b672-b74357cd9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import set_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca6b2b9-6a22-4e03-9759-85567da5b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1607e2-e9ce-4a05-97ff-714383541c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d052741-56ed-4b5a-9b4b-04ba97467a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # openAI LLM\n",
    "# llm = OpenAI()\n",
    "\n",
    "# # This gives a rate time error saying that we need to pay to run this model\n",
    "# summary = llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41354f1-ced7-42c7-a1a8-2e9ce10b7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an opensource model - from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ced188c-9974-4d0a-9977-b3a3ac4da8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7d41b0-028e-4335-8f1e-de853a7f1953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.fake import FakeListLLM\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "fake_llm.invoke(\"Hi and goodbye, FakeListLLM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6399dae5-cb19-49da-810a-cfe4030da745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "# from langchain.schema import HumanMessage\n",
    "\n",
    "# llm = ChatOpenAI(model_name='gpt-4-0613')\n",
    "# response = llm.invoke([HumanMessage(content='Say \"Hello world\" in Python.')])\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207966f9-d61b-48e5-85ca-f8388e7a76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import SystemMessage\n",
    "\n",
    "# chat_output = llm.invoke([ \n",
    "#     SystemMessage(content=\"You're a helpful assistant\"), \n",
    "#     HumanMessage(content=\"What is the purpose of model regularization?\") \n",
    "# ])\n",
    "# print(chat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68641cb-cdd0-48b8-8b5d-968c39bdd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# llm = ChatAnthropic(\n",
    "#     model='claude-3-opus-20240229',\n",
    "# )\n",
    "\n",
    "# response = llm.invoke([HumanMessage(content='What is the best large language model?')])\n",
    "\n",
    "# print(response)\n",
    "# # please note that this needs a sufficient credit balance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84614c7f-f553-4063-a03f-714f8337c9e2",
   "metadata": {},
   "source": [
    "Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6526b8c-14ad-40c6-b80d-9dbb82fe916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"\n",
    "# Summarize this text in one sentence:\n",
    "\n",
    "# {text}\n",
    "# \"\"\"\n",
    "\n",
    "# llm = OpenAI()\n",
    "# summary = llm(prompt.format(text=\"Some long story\"))\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d4cb58b-25b9-4f56-b1ea-0cb917678a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc02f6d-8f00-4220-a8d7-32511e0ef243",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38461487-1c99-481a-b504-2f31f393f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a753222-0329-48a3-be86-0967fc1b56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b455817d-9a4f-4f5e-93f7-5d298dede2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a funny joke about chickens.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a01b48b-14e5-4b59-95b5-28d8eb8538c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI \n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# # Define a ChatPromptTemplate to translate text\n",
    "# template = ChatPromptTemplate.from_messages([\n",
    "#     ('system', 'You are an English to French translator.'),\n",
    "#     ('user', 'Translate this to French: {text}')\n",
    "# ])\n",
    "# llm = ChatOpenAI()\n",
    "# # Translate a joke about light bulbs\n",
    "# response = llm.invoke(template.format_messages(text='How many programmers does it take to change a light bulb?'))\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e525b215-7df3-41aa-9069-32a217a3d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_vertexai import ChatVertexAI\n",
    "# #from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# llm = ChatVertexAI(model_name=\"gemini-pro\")\n",
    "# #llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# template = \"\"\"Question: {question}\n",
    "# Answer: Let's think step by step.\"\"\"\n",
    "# prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "# question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "# llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeb43ae3-b897-45cd-94c3-d5ec54d7fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/statsgeneral/gayara/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema import StrOutputParser\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_length=128, temperature=0.5,\n",
    ")\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" \n",
    "prompt = PromptTemplate.from_template(template) \n",
    "runnable = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de28385-598a-4029-83b2-02e2bf4fccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UNL stands for the University of Nebraska-Lincoln. The University of Nebraska system was established in 1869. However, UNL in Lincoln was not established until later. The Morrill Act of 1862 provided for land-grant colleges, and Nebraska received its land grant in 1867. The University of Nebraska-Lincoln was officially established in 1869, but classes did not begin until 1871. So, the year UNL was established was 1871.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which year was UNL established? \" \n",
    "summary = runnable.invoke({\"question\": question})\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_llm_env)",
   "language": "python",
   "name": "tf_llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
